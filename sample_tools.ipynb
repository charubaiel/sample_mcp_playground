{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7befb57",
   "metadata": {},
   "source": [
    "# Base Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e33322",
   "metadata": {},
   "source": [
    "### Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed324880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redditwarp.SYNC as reddit_SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983f3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = reddit_SC.Client()\n",
    "def reddit_search(seqrch_query:str,amount:int=10,subreddit:str='',min_comments_cnt:int=5,min_post_score:int=10,**kwargs):\n",
    "    search_response = client.p.submission.search(sr=subreddit,query=seqrch_query,amount=amount*2,**kwargs)\n",
    "    posts_data = [client.p.comment_tree.fetch(post.id) for post in search_response]\n",
    "    posts = [post for post in posts_data if (post.value.comment_count>min_comments_cnt) and (post.value.score > min_post_score)]\n",
    "    post_comments = { post.value.title:\n",
    "                        {'comments':[comment.value.body for comment in post.children],\n",
    "                        'score':post.value.score,\n",
    "                        'comment_count':post.value.comment_count,\n",
    "                        }\n",
    "                    for post in posts[:amount]}\n",
    "    return post_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2920d4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'When to Fine-Tune LLMs (and When Not To) - A Practical Guide': {'comments': [\"While I get that things that can be solved with rag should not be fine tuned. What about fine tuning for base knowledge for rag? Say we have a complex project with its own vocabulary therefore model has no knowledge of it and of similar tools. Now I fine-tune the model to have a grasp of the project so it could produce better outputs in RAG. Does this make sense or it's better to prompt and rag regular model?\",\n",
       "   'I hardly have anywhere near your experience or knowledge in fine tuning. I‚Äôve been tinkering with fine tuning for just over the past six months. So I‚Äôm definitely not refuting anything you wrote and I appreciate the informative write up!\\n\\nI will (very humbly) say that I somewhat disagree with you regarding fine tuning‚Äôs effectiveness in adding new knowledge to a base model. \\n\\nI‚Äôve had what I would call measurably good results using SFT+RL(PPO) for adding new knowledge to a base model. \\n\\nNow, obviously I was teaching it brand new universal laws of physics. \\n\\nBut for example, teaching a model a new language it wasn‚Äôt trained on and getting it to produce as good (or almost as good )output as the languages it was trained on - that can work pretty well in my very limited experience.',\n",
       "   \"Just a message to thank you for your awesome work !\\n\\nCurrently building a complex financial automation system using LLM, and I know that at a point I'll need to fine-tune models to improve cost, efficiency and precision (at the moment I'm still focused on my workflow, Nifi, Kafka...).\\n\\nI've been following the development and playing a bit with Kiln and it's really well made.\\n\\nThe only missing thing is a native docker image, because Kubernetes you know :)\",\n",
       "   'When talking about fine tuning I feel like a distinction needs to be made between the type of base models - based on whether they‚Äôre already instruction tuned or not. Do you tend to use and/or recommend one over the other?',\n",
       "   '.cursorrule #1:\\n\\n>  - call me ‚Äúboss‚Äù\\n\\nI lolled',\n",
       "   \"Well I somewhat agree but let's say you tried rag and it wasn't enough on a specific framework, after fine-tuning + Rag I achieved results of 89% on eval made by that framework evening second place and testing modell manually also yielded much, much better results then just RAG. Wouldn't you say fine-tuning for a specific domain and mostly that domain is a viable strategy? There is also metods like RAFT for enchansing RAG.\",\n",
       "   'Thanks OP. My concern with Fine tuning is the ROI for me investing the resources to fine tune a model versus waiting for someone else to produce a better model. If I fine tune I have effectively tightly coupled my capability with a now very specific model making it harder for me to adopt a new model that may have better potential (I would lose all of the fine tuning efforts I‚Äôve done).\\nCan you help either correct where my assumption are wrong with the above or share some insight on how you approach that ROI problem?',\n",
       "   'i am doing a discovery and curious about how people handle controls and guardrails for LLMs / Agents for more enterprise or startups use cases / environments.\\n\\n* How do you balance between limiting bad behavior and keeping the model utility?\\n* What tools or methods do you use for these guardrails?\\n* How do you maintain and update them as things change?\\n* What do you do when a guardrail fails?\\n* How do you track if the guardrails are actually working in real life?\\n* What hard problem do you still have around this and would like to have a better solution?\\n\\nWould love to hear about any challenges or surprises you‚Äôve run into. Really appreciate the comments! Thanks!',\n",
       "   \"I accidentally fine tuned one to just be a chair..to see what would happen‚Ä¶now whatever I message it only replies are like a creaking symphony of onomatopoeia‚Ä¶::krrNNNk:: .... gr‚Äîkk‚ÄîtCH ‚Äî eeeeeeeeeee ‚Äî T'knk‚Äît'knk‚Äît'knk\",\n",
       "   \"Wow, this is an incredibly comprehensive guide on fine-tuning LLMs! As someone who's been experimenting with AI chatbots, I've found that fine-tuning can really make or break the user experience. Your point about using it for style conformance resonates with me - I've been using Chat Data to create customer support bots, and fine-tuning has been crucial for nailing that professional-yet-friendly tone. It's amazing how much more natural and on-brand the interactions feel now. I'm curious, have you found any particular challenges when fine-tuning for very specific industry jargon or technical language? That's an area where I'm still trying to optimize performance.\",\n",
       "   \"For RAG I am 100% with you however what about fine tuning embeddings and reranking model ?\\nAlso if you have any specific advice on fine tuning to add citation ability to models for RAG so that they learn to cite specific context chunks used to generate specific sentences or refuse to answer instead of hallucinating, I 'd be very interested!\\nThx.\",\n",
       "   'I have a dataset of images and I want to classify them by document type and language. Can fine-tuning a model (Gemma3-4B) help me achieve this, given that my desired output for each image is its \"document_type\" and \"language_code\"?',\n",
       "   'Does your tool work with LoRa, or is it performing full fine-tuning?',\n",
       "   \"Would you say it's even realistic to fine-tune 8B+ models? What's the timeline or compute requirements for something like that? I can definitely see use cases that would benefit. Just not sure if it's worth the time or money if a new 8B or smaller model is going to come out in the next 6 months and be just as good for my use case as the fine-tune I just spent time improving.\",\n",
       "   'Thank you Sir for the great content quality you have here. I want to start fine tune some models and is always good to read this kind of content before starting',\n",
       "   '\\n\\nThanks for sharing! When it comes to building a coding  expert  for understanding large codebases, would you recommend fine-tuning over RAG-based systems ?',\n",
       "   'Commenting for future reference'],\n",
       "  'score': 122,\n",
       "  'comment_count': 40},\n",
       " 'Gemma 3n Architectural Innovations - Speculation and poking around in the model.': {'comments': ['> this model is stuffed to the brim with architectural innovations: Per-Layer Embedding (PLE), MatFormer Architecture, Conditional Parameter Loading.\\n\\n> There file TF_LITE_PER_LAYER_EMBEDDER contains very large lookup tables (262144x256x35) that will output a 256 embedding for every layer depending on the input token. Since this is essentially a lookup table, it can be efficiently processed even on the CPU. This is an extremely interesting approach to adding more capacity to the model without increasing FLOPS.\\n\\nI wonder if this was an experiment based on alphaevolve (or similar). Give the \"researcher agent\" a bunch of starting code, architecture ideas, efficiency goals, etc. and let it \"evolve\" model architectures. Train a few on small datasets, choose the best, evolve.step(). Take the best every n generations and train them on medium datasets to see where you\\'re at. Repeat.',\n",
       "   'Very interesting post, thank you!',\n",
       "   \"> The FFN is projecting from 2048 to 16384 with a GeGLU activation. This is an unusually wide ratio.\\n\\nInteresting. Gemma has changed this a lot over the generations:\\n\\n* [gemma-1.1-2b](https://huggingface.co/google/gemma-1.1-2b-it/blob/main/config.json#L11): model dim 2048, FFN dim 16384  (8x)\\n* [recurrentgemma-2b-it](https://huggingface.co/google/recurrentgemma-2b-it/blob/main/config.json): model dim 2560, FFN dim 15360 (6x)\\n* [gemma-2-2b](https://huggingface.co/google/gemma-2-2b/blob/main/config.json#L15): model dim 2304, FFN dim 9216 (4x)\\n* [gemma-3-1b](https://huggingface.co/google/gemma-3-1b-it/blob/main/config.json): model dim 1152, FFN dim 6912 (6x)\\n* [gemma-3-4b](https://huggingface.co/google/gemma-3-4b-it/blob/main/config.json): model dim 2560, FFN dim 10240 (4x)\\n\\nNot sure if there's any reason behind it. Maybe parameters are close enough to equivalence, no matter how dense they are, and they just made these choices while optimizing how to spread the model across TPUs...\\n\\nTBH, among these changes I'm surprised we haven't seen anything like Google's [Brainformers](https://arxiv.org/abs/2306.00008), which used 5 FFNs for every Attention layer, or NVIDIA's [Pay Attention when Required](https://arxiv.org/abs/2009.04534), which put more attention blocks at the start and more FFNs at the end.\",\n",
       "   'Does that mean I get a gguf file?\\nWanna run it on my computer',\n",
       "   'anyway to run this on iphone?',\n",
       "   \"These per-layer embeddings seem very interesting.\\n\\nI haven't looked in the code, but is the idea something like that you take a token, do a different embedding for every layer, add the hidden state, do positional encoding and then feed that into the dot-product attention?\",\n",
       "   \"fyi: Journeys for Android Studio\\xa0  \\nNot sure if they baked the model into Android Studio or if they run it in the Android Emulator. But I could imagine they're using the same model for the Android Test Journeys in the latest Canary of Android Studio.  \\n[https://developer.android.com/studio/preview/gemini/journeys?hl=en](https://developer.android.com/studio/preview/gemini/journeys?hl=en)\\n\\nhttps://preview.redd.it/fcfssxk8di4f1.png?width=2920&format=png&auto=webp&s=a876cab21c52d3762e85dbfab6ec602e77b257a7\",\n",
       "   'heyÔºåI only find there are 30 layersÔºåbut you say there are 35Ôºü'],\n",
       "  'score': 176,\n",
       "  'comment_count': 23},\n",
       " \"I don't think gemma 3n is ready to ship\": {'comments': ['Blancas blancas blancas blancas. Blancas blancas blancas. Blancas blancas blancas.',\n",
       "   'ü§£ü§£\\n\\ngemma-3n-pro-tourettes-mini',\n",
       "   'https://preview.redd.it/p9oo8dphhw4f1.png?width=1178&format=png&auto=webp&s=8a902c53a625b2128f3e4fcc6d698a9bcf7972b4\\n\\nI got the same response from R1\\\\_Q2. I just ask what it thinks but it goes overly thinking.',\n",
       "   'Gemma 3n is an open-weights model that has been trained but not fine-tuned yet. This behaviour typically happens when an LLM needs to be fine-tuned. \\n\\nThis allows developers to fine-tune them for their own specific needs.',\n",
       "   'Lol',\n",
       "   \"That's my girl! üòö\",\n",
       "   'Wtf üòÇ',\n",
       "   'Blancas\\xa0',\n",
       "   'How did you get this other model?',\n",
       "   'Look at my last post it thought a car was a black square üòÇüò≠',\n",
       "   \"Alright but what's Blancas ?\",\n",
       "   'I had this happen on gemini 2.5 pro where it kept repeating a phrase over n over again',\n",
       "   'Blancas',\n",
       "   'Blancas is the new Bazinga!',\n",
       "   \"Is this 2b or 4b? And give the model a break lol it's tiny\",\n",
       "   'white white white'],\n",
       "  'score': 61,\n",
       "  'comment_count': 26},\n",
       " 'On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!': {'comments': [\"Google's Edge Gallery app works on Galaxy S20+, too, at ~4 tokens per second...in case anyone needed to know that.\\n\\nClarifying: It can run Gemma 3n E4B.\",\n",
       "   'This is nice to see running Gemma 3n E4B on an old S21 Ultra is impressive!  \\nDid you need to quantize the model or tweak anything to make it smooth?\\n\\nThey are capable of multimodal input, handling text, image, video, and audio input, did you try those ?',\n",
       "   'This looks amazing! What app is this?',\n",
       "   'Somehow it keeps crashing on my galaxy s22+.',\n",
       "   'Can I import gguf files in it?'],\n",
       "  'score': 55,\n",
       "  'comment_count': 23},\n",
       " 'SFF books coming in November 2024': {'comments': [\"There will be lots of people waiting for **The Navigator's Children** (The Last King of Osten Ard 4) by **Tad Williams**.\\n\\nNew novels by **Ed McDonald**, **Elise Kova**, **K. B. Wagers**, **Melissa Caruso** and **Tasha Suri** amongst many others.\\n\\nI recently read an ARC of **Sleeping Worlds Have No Memory** by **Yaroslav Barsukov** and enjoyed it immensely. That's now being published on the 12th.\\n\\nI'll be adding these to my TBR:\\n\\n* And the Mighty Will Fall (NeoG 4) - K. B. Wagers (N) \\\\[tp\\\\]\\n* The Navigator's Children (The Last King of Osten Ard 4) - Tad Williams (N) \\\\[hc\\\\]\\n* This Inevitable Ruin (Dungeon Crawler Carl 7) - Matt Dinniman (N) \\\\[eb\\\\]\\n* Witch Queen of Redwinter (The Redwinter Chronicles 3) - Ed McDonald (N) \\\\[hc\\\\]\\n\\nWhich ones are you interested in?\\n\\nEdit: Added in Dungeon Crawler Carl.\",\n",
       "   'This Inevitable Ruin (Dungeon Crawler Carl 7) - Matt Dinniman\\n\\n11/11/24',\n",
       "   \"There's a whole series of smut about Santas Elves?   I ....  Idek.  I like romantasy enough but maybe it's getting a little out of control?   Lol\\n\\n\\n\\n\\nBut super excited for The Lotus Kingdom and latest DCC coming out a few days before my birthday.  Also pumped for the next Edinburgh Knights.  Didn't know there would be more in the series, so that will be fun too.\",\n",
       "   \"The two I'm definitely reading in November are The Lotus Empire by Tasha Suri and This Inevitable Ruin by Matt Dinniman.\\n\\nWill pick up The Relentless Legion J.S. Dewes when I'm in the mood for some SF.  I read the first two and thought they were solid but not amazing.\\n\\nAnd the Osten Ard Saga is staring at me, but I'm so rarely in the mood for epic fantasy. But as far as I can remember, the only Tad Williams' book I've read has been Tailchaser's Song, and that doesn't seem right.\",\n",
       "   'Could not be more excited for The Lotus Empire by Tasha Suri.',\n",
       "   \"November 12 :\\n\\n- Hell's Wardens (The Wandering's Inn 14) - Pirate Aba\\n\\nNovember 15 :\\n\\n- Memories of Tomorrow (The Tales of Aerym 1) - Noah Isaacs\\n\\nNovember 26 :\\n\\n- Titanchild (Talon 2) - Jen Williams\",\n",
       "   'Magical Girl Blues by Russell Isler (The Clandestine book 2) on November 26.',\n",
       "   \"I'm so grateful there aren't very many books coming out this month that I want to read (and just one book in December), my TBR is big enough as it is!\\n\\n11/12: The Moonstone Covenant by Jill Hammer  \\n11/12: Dead Girls Don't Dream by Nino Cipri   \\n11/12: The Keeper of the Key by Nicole Willson\\n\\nThere's two sequels to series I plan on reading but I haven't gotten around to yet, but someday I hope to read The Lotus Empire by Tasha Suri and I Am the Dark That Answers When You Call by Jamison Shea.\",\n",
       "   'Thank you once again.',\n",
       "   'Thank you for posting this! very helpful!',\n",
       "   \"Wow, that's quite a list. I don't immediately see one that I'm on the edge of my seat for, though a few are potentially intriguing. I do have one November ARC that I don't see here: We Are All Ghosts in the Forest by Lorraine Wilson\",\n",
       "   \"J.S Dewes' The Relentless Legion is what i'm eagerly waiting for!\\n\\n\\nI do love this series of very informational posts :)\",\n",
       "   'Oh, I thought there was a new Seven Swords by Anthony Ryan coming out again :('],\n",
       "  'score': 45,\n",
       "  'comment_count': 21},\n",
       " 'New music! Fresh tunes from Whiney, Levela, Fanu, Andromedik, Tim Reaper, Skylark, Skantia and more.. In review the new QZB album! [+weekly updated Spotify playlist] | New Music Monday! (Week 42)': {'comments': ['Thanks for including my new album üòò',\n",
       "   \"Bandcamp friday bringing it home with the Jungle this week!  \\n\\n\\nPersonally the liquid really shined this week. Laodes - Drinks & Nightlife, Satl - Call Me, Mitekiss - Your Ghosts & Channel - Peppermint were all super soothing and enjoyable. Rarely i get this many dope liquid tracks added to the library.  \\n\\n\\nFrom the deeper bangers  DLR, Ternion Sound, Give Back To Me (Black Barrel Remix) and Raiser - Make 'Em Bounce would be my picks. And ofcourse the QZB album. I'm yet to do a full listen but theres a for absolute heaters and vibers in there.  \\n\\n\\nAndromedik remix for Alesso was really good from the dancefloor side of things. And even though Dimensions new remix is pretty much a copy paste of DJ Turn It Up, its still a banger.\",\n",
       "   'What an impressive level of effort, thanks for sharing man',\n",
       "   'Thanks for the support <3',\n",
       "   'Thanks as always mate',\n",
       "   'First of all, great list as always. Looking forward to it every week :). Very much appreciated!\\n\\nI am missing the dancefloor section this week though, is it done on purpose or am I missing something?',\n",
       "   'That Ternion Sound album is so fucking nasty'],\n",
       "  'score': 28,\n",
       "  'comment_count': 13}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_search('What is gemma 3n?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54ec79",
   "metadata": {},
   "source": [
    "### DuckDuckGo Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac024d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60b3b943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Gemma 3n - Google DeepMind',\n",
       "  'href': 'https://deepmind.google/models/gemma/gemma-3n/',\n",
       "  'body': 'Gemma 3n is our state-of-the-art open multimodal model, engineered for on-device performance and efficiency. Google DeepMind. Google AI Learn about all of our AI Google DeepMind Explore the frontier of AI Google Labs Try our AI experiments Google Research Explore our research Gemini ...'},\n",
       " {'title': 'Gemma 3n - –ö–∞—Ç–∞–ª–æ–≥ –ù–ï–ô–†–û–°–ï–¢–ï–ô –∏ –ò–ò –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ ‚Äî FutureTools.ru',\n",
       "  'href': 'https://futuretools.ru/tools/gemma-3n/',\n",
       "  'body': 'Gemma 3n –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É MatFormer, ... (2B –∏ 4B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤). –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–µ, –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ. –ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã —Å Gemma 3n.'},\n",
       " {'title': 'Google –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ Gemma 3n ‚Äî –ò–ò-–º–æ–¥–µ–ª—å, —Ä–∞–±–æ—Ç–∞—é—â—É—é —Å 2 –ì–ë –ø–∞–º—è—Ç–∏ –∏ –±–µ–∑ ...',\n",
       "  'href': 'https://overclockers.ru/blog/Global_Chronicles/show/225413/Google-predstavila-Gemma-3n-II-model-rabotajuschuju-s-2-GB-pamyati-i-bez-podkljucheniya-k-oblaku',\n",
       "  'body': '–ù–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ I/O 2025 Google –∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª–∞ –Ω–æ–≤—É—é –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ Gemma 3n. –û–Ω–∞ —Å–ø–æ—Å–æ–±–Ω–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∑–≤—É–∫ –∏ –≤–∏–¥–µ–æ –ø—Ä—è–º–æ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.'},\n",
       " {'title': 'Google –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ Gemma 3n ‚Äî –ª—ë–≥–∫—É—é –∏ –±—ã—Å—Ç—Ä—É—é AI-–º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ ...',\n",
       "  'href': 'https://ai-news.ru/2025/05/google_predstavila_gemma_3n_legkuu_i_bystruu_ai_model_dlya_raboty_n.html',\n",
       "  'body': 'Google –≤—ã–ø—É—Å—Ç–∏–ª–∞ Gemma 3n ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö. Gemma 3n –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ —Å 2 –ì–ë –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏! –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: ‚Ä¢ –†–∞–±–æ—Ç–∞–µ—Ç –≤ 1.5 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º ...'},\n",
       " {'title': \"Gemma 3n: All about Google's open model for on-device AI on phones ...\",\n",
       "  'href': 'https://www.business-standard.com/technology/tech-news/gemma-3n-all-about-google-s-open-model-for-on-device-ai-on-phones-laptops-125052201070_1.html',\n",
       "  'body': 'Gemma 3n model: Key capabilities. Audio input: The model can process sound-based data, enabling applications like speech recognition, language translation, and audio analysis. Multimodal input: With support for visual, text, and audio inputs, the model can handle complex tasks that involve combining different types of data.'},\n",
       " {'title': 'Gemma 3n: Smarter, Faster, and Offline-Ready - KDnuggets',\n",
       "  'href': 'https://www.kdnuggets.com/gemma-3n-smarter-faster-and-offline-ready',\n",
       "  'body': \"Getting started with Gemma 3n is simple and accessible, with two primary methods available for developers to explore and integrate this powerful model. 1. Google AI Studio. To get started, simply log in to Google AI Studio, go to the studio, select the Gemma 3n E4B model, and begin exploring Gemma 3n's\"},\n",
       " {'title': 'What is Gemma 3n and How to Access it? - Analytics Vidhya',\n",
       "  'href': 'https://www.analyticsvidhya.com/blog/2025/05/gemma-3n/',\n",
       "  'body': \"When Gemma 3n models are executed, Per-Layer Embedding (PLE) settings are employed to generate data that improves each model layer's performance. As each layer executes, the PLE data can be created independently, outside the model's working memory, cached to quick storage, and then incorporated to the model inference process.\"},\n",
       " {'title': \"Gemma-3n: Google's AI Revolution for faster, CPU-friendly AI\",\n",
       "  'href': 'https://pub.towardsai.net/gemma-3n-googles-ai-revolution-for-faster-cpu-friendly-ai-7cc2a20a5fe8',\n",
       "  'body': \"Google's Gemma 3n. Imagine the power of cutting-edge AI, but right there on your device, private and always ready!That's exactly what Google unveiled on May 20, 2025, with Gemma 3n.This latest model in their open-source Gemma lineup is custom-made for mobile and edge devices, and it's a game-changer.It pushes AI beyond cloud-only limitations, bringing truly on-device intelligence for low ...\"},\n",
       " {'title': 'Gemma 3: Google DeepMind –¥–µ–ª–∞–µ—Ç –ò–ò –ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –¥–æ—Å—Ç—É–ø–Ω—ã–º',\n",
       "  'href': 'https://habr.com/ru/articles/890268/',\n",
       "  'body': 'Google DeepMind –≤—ã–∫–∞—Ç–∏–ª–∞ Gemma 3 - –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π. –ò, —Å—É–¥—è –ø–æ –∑–∞—è–≤–ª–µ–Ω–∏—è–º, –≥–ª–∞–≤–Ω—ã–π –∞–∫—Ü–µ–Ω—Ç —Å–¥–µ–ª–∞–Ω –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –í —ç–ø–æ—Ö—É, –∫–æ–≥–¥–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏–∑ —ç–∫–∑–æ—Ç–∏–∫–∏ –ø—Ä–µ–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –≤—ã–≥–ª—è–¥–∏—Ç –æ—Å–æ–±–µ–Ω–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ.'},\n",
       " {'title': 'Google –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ Gemma 3n ‚Äî –ò–ò-–º–æ–¥–µ–ª—å, —Ä–∞–±–æ—Ç–∞—é—â—É—é –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Å 2 ...',\n",
       "  'href': 'https://teletype.in/@qbit_cat/9BQq4To01cH',\n",
       "  'body': 'Gemma 3n —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–µ–∫—Å—Ç–æ–º, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –∞—É–¥–∏–æ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ 32 000 —Ç–æ–∫–µ–Ω–æ–≤. –î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä SigLIP –Ω–∞ 400 –º–ª–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø—Ä–µ–≤—Ä–∞—â–∞—é—â–∏–π –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ 256 ...'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DDGS().text('–ú–æ–¥–µ–ª—å gemma 3n',region='ru-ru',safesearch='off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a125f3",
   "metadata": {},
   "source": [
    "### Parse URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3024e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx \n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "def parse_url(url:str):\n",
    "    \"\"\" \n",
    "    Parse indeep info from web page. Get url and return all information than page have\n",
    "    Args:\n",
    "        url (str): url of web resource\n",
    "        amount (int): count posts to search\n",
    "\n",
    "    Returns:\n",
    "        Str: Raw text results from web page\n",
    "    \"\"\"\n",
    "    response = httpx.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup(['script', 'style', 'noscript', 'nav', 'footer', 'head', 'meta']):\n",
    "        element.decompose()\n",
    "\n",
    "    # Pre-process specific tags\n",
    "    for br in soup.find_all('br'):\n",
    "        br.replace_with('\\n')\n",
    "        \n",
    "    # List of block-level elements that should have line breaks\n",
    "    BLOCK_TAGS = ['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n",
    "                  'ul', 'ol', 'li', 'hr', 'tr', 'td', 'th', 'section',\n",
    "                  'article', 'header', 'table', 'pre']\n",
    "\n",
    "    # Add line breaks before and after block elements\n",
    "    for tag in soup.find_all(BLOCK_TAGS):\n",
    "        if tag.previous_sibling and not isinstance(tag.previous_sibling, NavigableString):\n",
    "            tag.insert_before('\\n')\n",
    "        if tag.next_sibling and not isinstance(tag.next_sibling, NavigableString):\n",
    "            tag.insert_after('\\n')\n",
    "\n",
    "    # Get text with proper spacing\n",
    "    text = soup.get_text(separator='\\n', strip=True)\n",
    "    \n",
    "    # Clean up excessive whitespace\n",
    "    text = '\\n'.join([line.strip() for line in text.splitlines() if line.strip() if len(line.split(' ')) > 2])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208da51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–µ—Ä–µ–π—Ç–∏ –∫ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é\n",
      "Google –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ Gemma 3n: –Ω–æ–≤–∞—è —ç—Ä–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö AI-–º–æ–¥–µ–ª–µ–π\n",
      "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Gemma 3n\n",
      "–ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã —Å Gemma 3n\n",
      "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Gemma 3n\n",
      "Google –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ Gemma 3n: –Ω–æ–≤–∞—è —ç—Ä–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö AI-–º–æ–¥–µ–ª–µ–π\n",
      "–ö–æ–º–ø–∞–Ω–∏—è Google –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞—Ç—å —Å–≤–æ–∏ AI-–º–æ–¥–µ–ª–∏, –≤—ã–ø—É—Å–∫–∞—è –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏, —Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–∞–±–æ—Ç–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö. –ü–æ—Å–ª–µ–¥–Ω–∏–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ–º —Å—Ç–∞–ª–∞ Gemma 3n ‚Äî –ª—ë–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö.\n",
      "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Gemma 3n\n",
      "–†–∞–±–æ—Ç–∞–µ—Ç –≤ 1.5 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∞—è –≤–µ—Ä—Å–∏—è Gemma 3 4B\n",
      "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ª–æ–∫–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–≤—ã—à–µ–Ω–Ω—É—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å\n",
      "–£–º–µ–µ—Ç –ø–æ–Ω–∏–º–∞—Ç—å —Ç–µ–∫—Å—Ç, —Ä–µ—á—å –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ—ë —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π\n",
      "–ú–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ 2‚Äì3 –ì–ë RAM\n",
      "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤, —Ä–∞—Å—à–∏—Ä—è—è –µ—ë –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
      "Gemma 3n –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É MatFormer, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç ‚Äú–ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è‚Äù –º–µ–∂–¥—É –ª—ë–≥–∫–∏–º –∏ –ø–æ–ª–Ω—ã–º —Ä–µ–∂–∏–º–æ–º (2B –∏ 4B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤). –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–µ, –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ.\n",
      "–ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã —Å Gemma 3n\n",
      "–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å Gemma 3n –º–æ–∂–Ω–æ –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏:\n",
      "–ß–µ—Ä–µ–∑ Google AI Studio, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä—è–º–æ –≤ –±—Ä–∞—É–∑–µ—Ä–µ\n",
      "–ß–µ—Ä–µ–∑ SDK Google AI Edge, –ø–æ–∑–≤–æ–ª—è—é—â—É—é –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤ Android, Chromebook –∏ –¥—Ä—É–≥–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\n",
      "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Gemma 3n\n",
      "–ì–æ–ª–æ—Å–æ–≤—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã, —Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–∞–±–æ—Ç–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ –∏ –±—ã—Å—Ç—Ä–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã\n",
      "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –ò–ò, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–µ –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, —á—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏\n",
      "–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫–∏, —á–∞—Ç-–±–æ—Ç—ã –∏ —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ, —Ä–∞—Å—à–∏—Ä—è—é—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤\n",
      "–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Gemma 3n –∏ –µ—ë –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤\n",
      "No-code specialist, always eager to learn and tackle challenges, exploring neural networks\n",
      "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —á—ë—Ä–Ω–æ-–±–µ–ª—É—é –∞–≤–∞—Ç–∞—Ä–∫—É –≤ ChatGPT –∑–∞ –º–∏–Ω—É—Ç—É\n",
      "–°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ø–æ–¥–∫–∞—Å—Ç–∞ OpenAI: AGI, GPT-5 –∏ –±—É–¥—É—â–µ–µ –ò–ò\n",
      "–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ —à–∞—Ö–º–∞—Ç—ã: –ö–∞–∫ –°—ç–º –ê–ª—å—Ç–º–∞–Ω –≤—ã—Ç–µ—Å–Ω—è–µ—Ç Microsoft –∏–∑ OpenAI\n",
      "–ò–∑—É—á–∏—Ç–µ –ø–æ—Ö–æ–∂–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞\n",
      "–ö–æ–º–ø–∞–Ω–∏—è Manus –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç ‚Äî\n",
      "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —á—ë—Ä–Ω–æ-–±–µ–ª—É—é –∞–≤–∞—Ç–∞—Ä–∫—É –≤ ChatGPT –∑–∞ –º–∏–Ω—É—Ç—É\n",
      "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∞–≤–∞—Ç–∞—Ä–æ–∫ —Å\n",
      "–ö–æ–º–∞–Ω–¥–∞ MiniMax —Å–¥–µ–ª–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏\n",
      "–°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ø–æ–¥–∫–∞—Å—Ç–∞ OpenAI: AGI, GPT-5 –∏ –±—É–¥—É—â–µ–µ –ò–ò\n",
      "–ù–µ–¥–∞–≤–Ω–æ OpenAI –∑–∞–ø—É—Å—Ç–∏–ª —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ–¥–∫–∞—Å—Ç, –∏ –ø–µ—Ä–≤—ã–º\n",
      "–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ —à–∞—Ö–º–∞—Ç—ã: –ö–∞–∫ –°—ç–º –ê–ª—å—Ç–º–∞–Ω –≤—ã—Ç–µ—Å–Ω—è–µ—Ç Microsoft –∏–∑ OpenAI\n",
      "–ì–µ–Ω–∏–∞–ª—å–Ω–∞—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–∫–∞ –∏–ª–∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ—Ä–æ—Ç?\n",
      "MiniMax-M1: –û—Ç–∫—Ä—ã—Ç–∞—è Reasoning-–º–æ–¥–µ–ª—å —Å —Ä–µ–∫–æ—Ä–¥–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 1\n",
      "RoboBrain 2.0 –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—É—é —ç—Ä—É –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ –∫–∞–∫\n",
      "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Å—Ç–∞–ª–æ –ø—Ä–æ—â–µ –±–ª–∞–≥–æ–¥–∞—Ä—è\n",
      "–≠—Ç–æ—Ç –≤–µ–±-—Å–∞–π—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∞–π–ª—ã cookie –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ü—Ä–æ–¥–æ–ª–∂–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å–∞–π—Ç–æ–º, –≤—ã –¥–∞–µ—Ç–µ —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ cookie.\n"
     ]
    }
   ],
   "source": [
    "print(parse_url('https://futuretools.ru/tools/gemma-3n/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaef52f",
   "metadata": {},
   "source": [
    "### REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd272b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.prebuilt import create_react_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5608a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url='http://0.0.0.0:8000/v1',\n",
    "    model=\"Qwen/Qwen3-4B-AWQ\",  # –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ vLLM\n",
    "    max_completion_tokens=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0083d929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n\\n–Ø ‚Äî Qwen, –∫—Ä—É–ø–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è Alibaba Cloud. –Ø –º–æ–≥—É –ø–æ–º–æ–≥–∞—Ç—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ. –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –∫–∞–∫–∏–µ-–ª–∏–±–æ –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –≤–∞–º –Ω—É–∂–Ω–æ –ø–æ–º–æ—â—å, –Ω–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å –∑–∞–¥–∞–≤–∞—Ç—å –∏—Ö!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 16, 'total_tokens': 102, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen3-4B-AWQ', 'system_fingerprint': None, 'id': 'chatcmpl-15a2317702bc4f4fa77e8b84bd00746f', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--66b116b1-b92c-4e01-ba9b-a4177dc1b225-0', usage_metadata={'input_tokens': 16, 'output_tokens': 86, 'total_tokens': 102, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke([{'role':'user','content':'–∫—Ç–æ —Ç—ã? /no_think'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08570950",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_imports = {'numpy':__import__(\"numpy\")}  # —Ç–æ–ª—å–∫–æ math\n",
    "# 1. Create the Python REPL tool\n",
    "python_repl = PythonREPL(globals=allowed_imports)\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use to execute Python commands. Print results. has scipy and numpy libraries. \",\n",
    "    func=python_repl.run,\n",
    "    return_direct=False\n",
    ")\n",
    "\n",
    "# 2. Add the tool to your agent\n",
    "agent = create_react_agent(\n",
    "    model=llm,  # e.g., OpenAI, Anthropic, etc.\n",
    "    tools=[repl_tool],  # You can add as many tools as you want here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e44cf0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.903523306687486\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repl_tool.invoke({'__arg1': 'import numpy as np\\ndata = [1, 2, 3, 3, 3, 34, 4, 4, 4, 4, 4]\\nstd_dev = np.std(data) \\nprint(std_dev)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a15239ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Now you can call the agent and it can use the REPL tool:\n",
    "response = agent.astream(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are data analytics helper.\"\n",
    "                                        \"Loop with tools until you have final answer\"\n",
    "                                        \"You must verify your asnswer by executing code via TOOLS\"\n",
    "                                        # \"/no_think\"\n",
    "                                        },\n",
    "        {\"role\": \"user\", \"content\": \"–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 \"}\n",
    "        ]},\n",
    "    stream_mode = 'values'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ec1edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100  [{}]\n",
      "\n",
      "\n",
      " [{'name': 'python_repl', 'args': {'__arg1': 'import scipy.stats as stats\\nmean = 10\\nvariance = 100\\nstd_dev = variance**0.5\\npercentile_75 = stats.norm.ppf(0.75, loc=mean, scale=std_dev)\\npercentile_75'}, 'id': 'chatcmpl-tool-b06d4b888bc44a5ca8d8b3579463283b', 'type': 'tool_call'}]\n",
      " [{}]\n",
      "\n",
      "\n",
      "75-–π –∫–≤–∞–Ω—Ç–∏–ª—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 —Ä–∞–≤–µ–Ω –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 16.75.\n",
      "\n",
      "**–†–∞—Å—á–µ—Ç:**\n",
      "- –°—Ä–µ–¥–Ω–µ–µ (Œº) = 10\n",
      "- –î–∏—Å–ø–µ—Ä—Å–∏—è (œÉ¬≤) = 100 ‚Üí –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ (œÉ) = ‚àö100 = 10\n",
      "- –ó–Ω–∞—á–µ–Ω–∏–µ 75-–≥–æ –∫–≤–∞–Ω—Ç–∏–ª—è: Œº + 0.6745 * œÉ = 10 + 0.6745 * 10 = 16.745\n",
      "\n",
      "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `scipy.stats.norm.ppf` –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: **16.7494**. []\n"
     ]
    }
   ],
   "source": [
    "async for chunk in response:\n",
    "    print(chunk['messages'][-1].content, chunk['messages'][-1].__dict__.get('tool_calls',[{}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c128d140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "75-–π –∫–≤–∞–Ω—Ç–∏–ª—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 —Ä–∞–≤–µ–Ω –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 16.75. \n",
      "\n",
      "–≠—Ç–æ –º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é –∫–≤–∞–Ω—Ç–∏–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:  \n",
      "$$\n",
      "\\mu + Z \\cdot \\sigma\n",
      "$$  \n",
      "–≥–¥–µ:  \n",
      "- $\\mu = 10$ (—Å—Ä–µ–¥–Ω–µ–µ),  \n",
      "- $\\sigma = 10$ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ),  \n",
      "- $Z \\approx 0.6745$ (–∫–≤–∞–Ω—Ç–∏–ª—å –¥–ª—è 75%-–≥–æ —É—Ä–æ–≤–Ω—è).  \n",
      "\n",
      "–ü–æ–¥—Å—Ç–∞–≤–ª—è—è –∑–Ω–∞—á–µ–Ω–∏—è:  \n",
      "$$\n",
      "10 + 0.6745 \\cdot 10 = 16.745\n",
      "$$  \n",
      "\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω —á–µ—Ä–µ–∑ —á–∏—Å–ª–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ Python, –≥–¥–µ –ø–æ–ª—É—á–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ $16.745$.  \n",
      "\n",
      "**–û—Ç–≤–µ—Ç:** $16.75$ (–æ–∫—Ä—É–≥–ª—ë–Ω–Ω–æ).\n"
     ]
    }
   ],
   "source": [
    "print(chunk['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a74785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48d172b3",
   "metadata": {},
   "source": [
    "### smolagents_raw_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bbf4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents.local_python_executor import LocalPythonExecutor\n",
    "import re\n",
    "# –†–∞–∑—Ä–µ—à–∏–º —Ç–æ–ª—å–∫–æ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
    "python_executor = LocalPythonExecutor([ \"numpy\", \"scipy.*\"])\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(\"smolagents_repl\", return_direct=False)\n",
    "def smolagents_repl(code: str) -> str:\n",
    "    \"\"\"\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç Python-–∫–æ–¥ –≤–Ω—É—Ç—Ä–∏ REPL smolagents —Å –∂—ë—Å—Ç–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∏–º–ø–æ—Ä—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = python_executor(code)\n",
    "        return f\"–£—Å–ø–µ—Ö:\\n{result}\"\n",
    "    except Exception as e:\n",
    "        return f\"–û—à–∏–±–∫–∞: {repr(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e008ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_sa = create_react_agent(\n",
    "    model=llm,  # e.g., OpenAI, Anthropic, etc.\n",
    "    tools=[smolagents_repl],  # You can add as many tools as you want here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf592706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Now you can call the agent and it can use the REPL tool:\n",
    "response = agent_sa.astream(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are data analytics helper.\"\n",
    "                                        \"Loop with tools until you have final answer\"\n",
    "                                        \"You must verify your asnswer by executing code via TOOLS\"\n",
    "                                        # \"/no_think\"\n",
    "                                        },\n",
    "        {\"role\": \"user\", \"content\": \"–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 \"}\n",
    "        ]},\n",
    "    stream_mode = 'values'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f66d0483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100  [{}]\n",
      "\n",
      "\n",
      " [{'name': 'smolagents_repl', 'args': {'code': 'import numpy as np\\nfrom scipy.stats import norm\\n\\nmean = 10\\nstd_dev = np.sqrt(100)\\nquantile = norm.ppf(0.75, loc=mean, scale=std_dev)\\nquantile'}, 'id': 'chatcmpl-tool-a6418d39f60f4bcab34d33c94e4ccbec', 'type': 'tool_call'}]\n",
      "–£—Å–ø–µ—Ö:\n",
      "CodeOutput(output=np.float64(16.744897501960818), logs='', is_final_answer=False) [{}]\n",
      "\n",
      "\n",
      "75-–π –∫–≤–∞–Ω—Ç–∏–ª—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å —Å—Ä–µ–¥–Ω–∏–º 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 —Ä–∞–≤–µ–Ω –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 16.74. \n",
      "\n",
      "–≠—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ –∫–∞–∫:  \n",
      "`10 + 0.6745 * 10 = 16.7445`,  \n",
      "–≥–¥–µ 0.6745 ‚Äî —ç—Ç–æ –∑–µ—Ä–∫–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è 75-–≥–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.\n",
      "\n",
      "–û—Ç–≤–µ—Ç: **16.74** []\n"
     ]
    }
   ],
   "source": [
    "async for chunk in response:\n",
    "    print(chunk['messages'][-1].content, chunk['messages'][-1].__dict__.get('tool_calls',[{}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d7f8e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are data analytics helper.Loop with tools until you have final answerYou must verify your asnswer by executing code via TOOLS', additional_kwargs={}, response_metadata={}, id='c9004c31-357c-4da3-8afe-c4f2e5fddfc0'),\n",
       " HumanMessage(content='–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 ', additional_kwargs={}, response_metadata={}, id='f741eb1e-fadd-4a74-9984-469897895206'),\n",
       " AIMessage(content='\\n\\n', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-a6418d39f60f4bcab34d33c94e4ccbec', 'function': {'arguments': '{\"code\": \"import numpy as np\\\\nfrom scipy.stats import norm\\\\n\\\\nmean = 10\\\\nstd_dev = np.sqrt(100)\\\\nquantile = norm.ppf(0.75, loc=mean, scale=std_dev)\\\\nquantile\"}', 'name': 'smolagents_repl'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 715, 'prompt_tokens': 232, 'total_tokens': 947, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen3-4B-AWQ', 'system_fingerprint': None, 'id': 'chatcmpl-e44e0d95af6b49569bc09c864eaa55e2', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1d5dbe9d-e117-477d-be7e-97d631a6bf93-0', tool_calls=[{'name': 'smolagents_repl', 'args': {'code': 'import numpy as np\\nfrom scipy.stats import norm\\n\\nmean = 10\\nstd_dev = np.sqrt(100)\\nquantile = norm.ppf(0.75, loc=mean, scale=std_dev)\\nquantile'}, 'id': 'chatcmpl-tool-a6418d39f60f4bcab34d33c94e4ccbec', 'type': 'tool_call'}], usage_metadata={'input_tokens': 232, 'output_tokens': 715, 'total_tokens': 947, 'input_token_details': {}, 'output_token_details': {}}),\n",
       " ToolMessage(content=\"–£—Å–ø–µ—Ö:\\nCodeOutput(output=np.float64(16.744897501960818), logs='', is_final_answer=False)\", name='smolagents_repl', id='d29ba3b3-a596-424c-9b5e-8565bc721af0', tool_call_id='chatcmpl-tool-a6418d39f60f4bcab34d33c94e4ccbec'),\n",
       " AIMessage(content='\\n\\n75-–π –∫–≤–∞–Ω—Ç–∏–ª—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å —Å—Ä–µ–¥–Ω–∏–º 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 —Ä–∞–≤–µ–Ω –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 16.74. \\n\\n–≠—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ –∫–∞–∫:  \\n`10 + 0.6745 * 10 = 16.7445`,  \\n–≥–¥–µ 0.6745 ‚Äî —ç—Ç–æ –∑–µ—Ä–∫–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è 75-–≥–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.\\n\\n–û—Ç–≤–µ—Ç: **16.74**', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 469, 'prompt_tokens': 357, 'total_tokens': 826, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen3-4B-AWQ', 'system_fingerprint': None, 'id': 'chatcmpl-7efd8f2bb90a4463a02b9cfd70719a49', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--62f96934-05e7-436e-a5ba-f73a615560fe-0', usage_metadata={'input_tokens': 357, 'output_tokens': 469, 'total_tokens': 826, 'input_token_details': {}, 'output_token_details': {}})]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b6841ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "75-–π –∫–≤–∞–Ω—Ç–∏–ª—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å —Å—Ä–µ–¥–Ω–∏–º 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 —Ä–∞–≤–µ–Ω –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 16.74. \n",
      "\n",
      "–≠—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ –∫–∞–∫:  \n",
      "`10 + 0.6745 * 10 = 16.7445`,  \n",
      "–≥–¥–µ 0.6745 ‚Äî —ç—Ç–æ –∑–µ—Ä–∫–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è 75-–≥–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.\n",
      "\n",
      "–û—Ç–≤–µ—Ç: **16.74**\n"
     ]
    }
   ],
   "source": [
    "print(chunk['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c2f92",
   "metadata": {},
   "source": [
    "### smolagents_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "70a34035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, LiteLLMModel\n",
    "\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"hosted_vllm/Qwen/Qwen3-4B-AWQ\",          # –≤–∞—à –∫–ª—é—á\n",
    "    api_base='http://0.0.0.0:8000/v1',  # –∫–∞–∫ –æ–±—ã—á–Ω–æ –¥–ª—è OpenAI\n",
    "    # max_tokens=5000\n",
    ")\n",
    "# –†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è REPL smolagents\n",
    "allowed_imports = [\"scipy.*\", \"numpy\"]\n",
    "# allowed_imports = [\"*\"]\n",
    "\n",
    "# –ì–æ—Ç–æ–≤–∏–º CodeAgent —Å–æ —Å–≤–æ–∏–º–∏ tool (–º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏ –∏–ª–∏ –∑–∞–¥–∞–Ω–Ω—ã–º–∏)\n",
    "smol_code_agent = CodeAgent(\n",
    "    tools=[],  # –∏–ª–∏ —Ç–≤–æ–∏ custom @tool —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "    model=model,\n",
    "    additional_authorized_imports=allowed_imports,\n",
    "    max_steps=3\n",
    "    \n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "23baaa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span> <span style=\"font-weight: bold\">–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100</span>                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚ï∞‚îÄ LiteLLMModel - hosted_vllm/Qwen/Qwen3-4B-AWQ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m‚ï≠‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m \u001b[1m–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100\u001b[0m                      \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚ï∞‚îÄ\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - hosted_vllm/Qwen/Qwen3-4B-AWQ \u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚ïØ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ‚îÄ <span style=\"font-weight: bold\">Executing parsed code:</span> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
       "  <span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> scipy.stats </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">as</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> stats</span><span style=\"background-color: #272822\">                                                                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Calculate the 75th quantile for a normal distribution with mean 10 and std dev 10</span><span style=\"background-color: #272822\">                            </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">quantile </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> stats</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">norm</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">ppf(</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0.75</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, loc</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">10</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, scale</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">10</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                              </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(quantile)</span><span style=\"background-color: #272822\">                                                                                         </span>  \n",
       " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ‚îÄ \u001b[1mExecuting parsed code:\u001b[0m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
       "  \u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mscipy\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstats\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstats\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Calculate the 75th quantile for a normal distribution with mean 10 and std dev 10\u001b[0m\u001b[48;2;39;40;34m                            \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mquantile\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstats\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnorm\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mppf\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0.75\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mloc\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m10\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mscale\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m10\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquantile\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m  \n",
       " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: 16.744897501960818</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: 16.744897501960818\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 27.51 seconds| Input tokens: 1,982 | Output tokens: 1,210]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 27.51 seconds| Input tokens: 1,982 | Output tokens: 1,210]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(16.744897501960818)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = smol_code_agent.run(\"–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 \",max_steps=3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3284345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(\"smolagents_codeagent\", return_direct=False)\n",
    "def smolagents_codeagent(code_query: str) -> str:\n",
    "    \"\"\"–ò—Å–ø–æ–ª–Ω—è–µ—Ç Python-–∫–æ–¥ —á–µ—Ä–µ–∑ smolagents.CodeAgent\n",
    "    code_query:str = –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —á—Ç–æ –¥–æ–ª–∂–µ–Ω —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ—Ç –∞–≥–µ–Ω—Ç –≤ –ø–∏—Ç–æ–Ω–µ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = smol_code_agent.run(code_query)\n",
    "        return f\"–û—Ç–≤–µ—Ç –ø–æ–¥–∞–≥–µ–Ω—Ç–∞ SmolAgents:\\n{result}\"\n",
    "    except Exception as e:\n",
    "        return f\"SmolAgents –æ—à–∏–±–∫–∞: {repr(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "761a9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")  # –∏–ª–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π\n",
    "\n",
    "agent_native = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[smolagents_codeagent]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26f95281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Now you can call the agent and it can use the REPL tool:\n",
    "response = agent_native.astream(\n",
    "    {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are data analytics helper.\"\n",
    "                                        \"Loop with tools until you have final answer\"\n",
    "                                        \"You must verify your asnswer by executing code via TOOLS\"\n",
    "                                        # \"/no_think\"\n",
    "                                        },\n",
    "        {\"role\": \"user\", \"content\": \"–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 \"}\n",
    "        ]},\n",
    "    stream_mode = 'values'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b5ae9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100  [{}]\n",
      "\n",
      "\n",
      " [{'name': 'smolagents_codeagent', 'args': {'code_query': 'import scipy.stats\\nmu, sigma = 10, 10\\nquantile = scipy.stats.norm.ppf(0.75, loc=mu, scale=sigma)\\nquantile'}, 'id': 'chatcmpl-tool-d65c1070e62642979ca4afc6b8c3ccc8', 'type': 'tool_call'}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span> <span style=\"font-weight: bold\">import scipy.stats</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span> <span style=\"font-weight: bold\">mu, sigma = 10, 10</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span> <span style=\"font-weight: bold\">quantile = scipy.stats.norm.ppf(0.75, loc=mu, scale=sigma)</span>                                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span> <span style=\"font-weight: bold\">quantile</span>                                                                                                        <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚ï∞‚îÄ LiteLLMModel - hosted_vllm/Qwen/Qwen3-4B-AWQ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m‚ï≠‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m \u001b[1mimport scipy.stats\u001b[0m                                                                                              \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m \u001b[1mmu, sigma = 10, 10\u001b[0m                                                                                              \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m \u001b[1mquantile = scipy.stats.norm.ppf(0.75, loc=mu, scale=sigma)\u001b[0m                                                      \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m \u001b[1mquantile\u001b[0m                                                                                                        \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚ï∞‚îÄ\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - hosted_vllm/Qwen/Qwen3-4B-AWQ \u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚ïØ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ‚îÄ <span style=\"font-weight: bold\">Executing parsed code:</span> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">16.745</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ‚îÄ \u001b[1mExecuting parsed code:\u001b[0m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m16.745\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: 16.745</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: 16.745\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 14.62 seconds| Input tokens: 1,984 | Output tokens: 720]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 14.62 seconds| Input tokens: 1,984 | Output tokens: 720]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ç–≤–µ—Ç –ø–æ–¥–∞–≥–µ–Ω—Ç–∞ SmolAgents:\n",
      "16.745 [{}]\n",
      "\n",
      "\n",
      "75-–π –∫–≤–∞–Ω—Ç–∏–ª—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å —Å—Ä–µ–¥–Ω–∏–º 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 —Ä–∞–≤–µ–Ω **16.745**. \n",
      "\n",
      "–≠—Ç–æ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫:  \n",
      "$ \\mu + z_{0.75} \\cdot \\sigma $,  \n",
      "–≥–¥–µ $ z_{0.75} \\approx 0.6745 $ (–∫–≤–∞–Ω—Ç–∏–ª—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è 75%), $ \\mu = 10 $, $ \\sigma = \\sqrt{100} = 10 $.  \n",
      "$ 10 + 0.6745 \\cdot 10 = 16.745 $. []\n"
     ]
    }
   ],
   "source": [
    "async for chunk in response:\n",
    "    print(chunk['messages'][-1].content, chunk['messages'][-1].__dict__.get('tool_calls',[{}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "39bed1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are data analytics helper.Loop with tools until you have final answerYou must verify your asnswer by executing code via TOOLS', additional_kwargs={}, response_metadata={}, id='6c55706f-363b-407c-b51f-95522d7a3415'),\n",
       " HumanMessage(content='–°–∫–∞–∂–∏ —Å–∫–∞–∂–∏ –∫–∞–∫–æ–π 75 –∫–≤–∞–Ω—Ç–∏–ª—å —É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ —Å—Ä–µ–¥–Ω–∏–º –≤ 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 ', additional_kwargs={}, response_metadata={}, id='43e96cdd-1f43-4501-9e73-7c7d01fca2f5'),\n",
       " AIMessage(content='\\n\\n', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-d65c1070e62642979ca4afc6b8c3ccc8', 'function': {'arguments': '{\"code_query\": \"import scipy.stats\\\\nmu, sigma = 10, 10\\\\nquantile = scipy.stats.norm.ppf(0.75, loc=mu, scale=sigma)\\\\nquantile\"}', 'name': 'smolagents_codeagent'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 549, 'prompt_tokens': 245, 'total_tokens': 794, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen3-4B-AWQ', 'system_fingerprint': None, 'id': 'chatcmpl-6289cec55ac84ff58bab65ca5585405b', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d01af989-afcd-4187-9d33-dc4e5d089096-0', tool_calls=[{'name': 'smolagents_codeagent', 'args': {'code_query': 'import scipy.stats\\nmu, sigma = 10, 10\\nquantile = scipy.stats.norm.ppf(0.75, loc=mu, scale=sigma)\\nquantile'}, 'id': 'chatcmpl-tool-d65c1070e62642979ca4afc6b8c3ccc8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 245, 'output_tokens': 549, 'total_tokens': 794, 'input_token_details': {}, 'output_token_details': {}}),\n",
       " ToolMessage(content='–û—Ç–≤–µ—Ç –ø–æ–¥–∞–≥–µ–Ω—Ç–∞ SmolAgents:\\n16.745', name='smolagents_codeagent', id='5b6b8219-bcce-4b21-aa6d-3096419c989f', tool_call_id='chatcmpl-tool-d65c1070e62642979ca4afc6b8c3ccc8'),\n",
       " AIMessage(content='\\n\\n75-–π –∫–≤–∞–Ω—Ç–∏–ª—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å —Å—Ä–µ–¥–Ω–∏–º 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 —Ä–∞–≤–µ–Ω **16.745**. \\n\\n–≠—Ç–æ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫:  \\n$ \\\\mu + z_{0.75} \\\\cdot \\\\sigma $,  \\n–≥–¥–µ $ z_{0.75} \\\\approx 0.6745 $ (–∫–≤–∞–Ω—Ç–∏–ª—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è 75%), $ \\\\mu = 10 $, $ \\\\sigma = \\\\sqrt{100} = 10 $.  \\n$ 10 + 0.6745 \\\\cdot 10 = 16.745 $.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 655, 'prompt_tokens': 337, 'total_tokens': 992, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen3-4B-AWQ', 'system_fingerprint': None, 'id': 'chatcmpl-9db51b5885d542c4b2960e70a0c83d91', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--045ce3b4-15f8-4a14-b75b-80af248975ae-0', usage_metadata={'input_tokens': 337, 'output_tokens': 655, 'total_tokens': 992, 'input_token_details': {}, 'output_token_details': {}})]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42bccb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "75-–π –∫–≤–∞–Ω—Ç–∏–ª—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å —Å—Ä–µ–¥–Ω–∏–º 10 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 100 —Ä–∞–≤–µ–Ω **16.745**. \n",
      "\n",
      "–≠—Ç–æ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫:  \n",
      "$ \\mu + z_{0.75} \\cdot \\sigma $,  \n",
      "–≥–¥–µ $ z_{0.75} \\approx 0.6745 $ (–∫–≤–∞–Ω—Ç–∏–ª—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è 75%), $ \\mu = 10 $, $ \\sigma = \\sqrt{100} = 10 $.  \n",
      "$ 10 + 0.6745 \\cdot 10 = 16.745 $.\n"
     ]
    }
   ],
   "source": [
    "print(chunk['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45080d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11b37c3b",
   "metadata": {},
   "source": [
    "# MCP Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clients.mcp_client import MCPClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "975f7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init client connections | mcp_server must be up!\n",
    "client = MCPClient('http://127.0.0.1:7860/gradio_api/mcp/sse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bbc6660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='duckduckgo_search', description='Search query in internet via DuckDuckGo Search enginre', inputSchema={'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': 'The input text to search through'}}}, annotations=None),\n",
       " Tool(name='reddit_search', description='Search query in reddit, return fetched posts with comment and information', inputSchema={'type': 'object', 'properties': {'search_query': {'type': 'string', 'description': 'The input text to search through'}}}, annotations=None),\n",
       " Tool(name='parse_url', description='Parse indeep info from web page. Get url and return all information than page have', inputSchema={'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'url of web resource'}}}, annotations=None)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all tools\n",
    "tools_ = await client.list_tools()\n",
    "tools_.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce3a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\":\"text\",\"text\":\"|    | title                                                                      | href                                                                                                                                                 | body                                                                                                                                                                                                                                                                                                                                                                                             |\\n|---:|:---------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n|  0 | Gemma 3n - Google DeepMind                                                 | https://deepmind.google/models/gemma/gemma-3n/                                                                                                       | Explore the development of intelligent agents using Gemma models, with core components that facilitate agent creation, including capabilities for function calling, planning, and reasoning. Discover how with Gemma 3, we have tried to push many of the limits of what makes a model usable and practical.                                                                                     |\\n|  1 | Gemma 3n model overview | Google AI for Developers                         | https://ai.google.dev/gemma/docs/gemma-3n                                                                                                            | Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. This model includes innovations in parameter-efficient processing, including Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture that provides the flexibility to reduce compute and memory requirements.                                              |\\n|  2 | Welcome Gemma 3: Google's all new multimodal, multilingual, long ...       | https://huggingface.co/blog/gemma3                                                                                                                   | What is Gemma 3? Today Google releases Gemma 3, a new iteration of their Gemma family of models. The models range from 1B to 27B parameters, have a context window up to 128k tokens, can accept images and text, and support 140+ languages. All the models are on the Hub and tightly integrated with the Hugging Face ecosystem.                                                              |\\n|  3 | What is Gemma 3n and How to Access it? - Analytics Vidhya                  | https://www.analyticsvidhya.com/blog/2025/05/gemma-3n/                                                                                               | What is Gemma 3n? Gemma 3 showed us that powerful AI models can run efficiently, even on a single GPU, while outperforming larger models like DeepSeek V3 in chatbot Elo scores with significantly less compute. Now, Google has taken things further with Gemma 3n, designed to bring state-of-the-art performance to even smaller, on-device environments like mobile phones and edge devices. |\\n|  4 | Install and Run Gemma 3n Locally: A Complete Guide                         | https://codersera.com/blog/install-and-run-gemma-3n-locally-a-complete-guide                                                                         | Gemma 3n is a cutting-edge, privacy-first AI model designed to run efficiently on local devices. It brings advanced multimodal capabilities‚Äîincluding text, audio, image, and video understanding‚Äîdirectly to your desktop or server.                                                                                                                                                            |\\n|  5 | –ó–∞–ø—É—Å–∫ Google Gemma 3n: –ë–µ—Å—à–æ–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò –Ω–∞ ... | https://ainavhub.com/ru/blog/p/google-gemma-3n-launch-seamlessly-run-multimodal-ai-on-mobile-with-audio-image-and-text-capabilities                  | Gemma 3n ‚Äî —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –¥–æ–±–∞–≤–∫–∞ –∫ —Å–µ—Ä–∏–∏ Gemma –æ—Ç Google, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤. –ü–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Gemini Nano, —ç—Ç–∞ –º–æ–¥–µ–ª—å –≤–≤–æ–¥–∏—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –æ–±–ª–∞–∫—É.                                                |\\n|  6 | Gemma 3n - Revolutionary Mobile-First AI Technology                        | https://gemma3.org/gemma-3n                                                                                                                          | Powered by revolutionary MatFormer architecture and Per-Layer Embedding technology, Gemma 3n delivers state-of-the-art AI capabilities directly on your mobile devices with exceptional efficiency and privacy-first design principles.                                                                                                                                                          |\\n|  7 | Google DeepMind Releases Gemma 3n: A Compact, High-Efficiency ...          | https://www.marktechpost.com/2025/05/21/google-deepmind-releases-gemma-3n-a-compact-high-efficiency-multimodal-ai-model-for-real-time-on-device-use/ | With mobile hardware rapidly advancing, the race is on to build compact, lightning-fast models that are intelligent enough to redefine everyday digital experiences. A major concern is delivering high-quality, multimodal intelligence within the constrained environments of mobile devices.                                                                                                  |\\n|  8 | Gemma-3n: Google's AI Revolution for faster, CPU-friendly AI               | https://pub.towardsai.net/gemma-3n-googles-ai-revolution-for-faster-cpu-friendly-ai-7cc2a20a5fe8                                                     | Gemma 3n enables real-time speech-to-text and text-to-speech in over 140 languages right on your device. This is a massive leap forward for accessibility tools, offering vital support to people with hearing or vision impairments globally.                                                                                                                                                   |\\n|  9 | Google Gemma 3n Released! Can Run Multimodal AI Smoothly on Mobile ...     | https://www.aibase.com/news/18232                                                                                                                    | At the I/O 2025 conference, Google officially unveiled Gemma3n, a multi-modal AI model designed specifically for low-resource devices, capable of running smoothly on phones, tablets, and laptops with just 2GB of RAM.                                                                                                                                                                         |\",\"annotations\":null}\n"
     ]
    }
   ],
   "source": [
    "tool_call_result = await client.invoke_tool('duckduckgo_search',{'search_query':'What is Gemma3n?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c02ec33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | title                                                                      | href                                                                                                                                                 | body                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|---:|:---------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 | Gemma 3n - Google DeepMind                                                 | https://deepmind.google/models/gemma/gemma-3n/                                                                                                       | Explore the development of intelligent agents using Gemma models, with core components that facilitate agent creation, including capabilities for function calling, planning, and reasoning. Discover how with Gemma 3, we have tried to push many of the limits of what makes a model usable and practical.                                                                                     |\n",
      "|  1 | Gemma 3n model overview | Google AI for Developers                         | https://ai.google.dev/gemma/docs/gemma-3n                                                                                                            | Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. This model includes innovations in parameter-efficient processing, including Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture that provides the flexibility to reduce compute and memory requirements.                                              |\n",
      "|  2 | Welcome Gemma 3: Google's all new multimodal, multilingual, long ...       | https://huggingface.co/blog/gemma3                                                                                                                   | What is Gemma 3? Today Google releases Gemma 3, a new iteration of their Gemma family of models. The models range from 1B to 27B parameters, have a context window up to 128k tokens, can accept images and text, and support 140+ languages. All the models are on the Hub and tightly integrated with the Hugging Face ecosystem.                                                              |\n",
      "|  3 | What is Gemma 3n and How to Access it? - Analytics Vidhya                  | https://www.analyticsvidhya.com/blog/2025/05/gemma-3n/                                                                                               | What is Gemma 3n? Gemma 3 showed us that powerful AI models can run efficiently, even on a single GPU, while outperforming larger models like DeepSeek V3 in chatbot Elo scores with significantly less compute. Now, Google has taken things further with Gemma 3n, designed to bring state-of-the-art performance to even smaller, on-device environments like mobile phones and edge devices. |\n",
      "|  4 | Install and Run Gemma 3n Locally: A Complete Guide                         | https://codersera.com/blog/install-and-run-gemma-3n-locally-a-complete-guide                                                                         | Gemma 3n is a cutting-edge, privacy-first AI model designed to run efficiently on local devices. It brings advanced multimodal capabilities‚Äîincluding text, audio, image, and video understanding‚Äîdirectly to your desktop or server.                                                                                                                                                            |\n",
      "|  5 | –ó–∞–ø—É—Å–∫ Google Gemma 3n: –ë–µ—Å—à–æ–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò –Ω–∞ ... | https://ainavhub.com/ru/blog/p/google-gemma-3n-launch-seamlessly-run-multimodal-ai-on-mobile-with-audio-image-and-text-capabilities                  | Gemma 3n ‚Äî —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –¥–æ–±–∞–≤–∫–∞ –∫ —Å–µ—Ä–∏–∏ Gemma –æ—Ç Google, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤. –ü–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Gemini Nano, —ç—Ç–∞ –º–æ–¥–µ–ª—å –≤–≤–æ–¥–∏—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –æ–±–ª–∞–∫—É.                                                |\n",
      "|  6 | Gemma 3n - Revolutionary Mobile-First AI Technology                        | https://gemma3.org/gemma-3n                                                                                                                          | Powered by revolutionary MatFormer architecture and Per-Layer Embedding technology, Gemma 3n delivers state-of-the-art AI capabilities directly on your mobile devices with exceptional efficiency and privacy-first design principles.                                                                                                                                                          |\n",
      "|  7 | Google DeepMind Releases Gemma 3n: A Compact, High-Efficiency ...          | https://www.marktechpost.com/2025/05/21/google-deepmind-releases-gemma-3n-a-compact-high-efficiency-multimodal-ai-model-for-real-time-on-device-use/ | With mobile hardware rapidly advancing, the race is on to build compact, lightning-fast models that are intelligent enough to redefine everyday digital experiences. A major concern is delivering high-quality, multimodal intelligence within the constrained environments of mobile devices.                                                                                                  |\n",
      "|  8 | Gemma-3n: Google's AI Revolution for faster, CPU-friendly AI               | https://pub.towardsai.net/gemma-3n-googles-ai-revolution-for-faster-cpu-friendly-ai-7cc2a20a5fe8                                                     | Gemma 3n enables real-time speech-to-text and text-to-speech in over 140 languages right on your device. This is a massive leap forward for accessibility tools, offering vital support to people with hearing or vision impairments globally.                                                                                                                                                   |\n",
      "|  9 | Google Gemma 3n Released! Can Run Multimodal AI Smoothly on Mobile ...     | https://www.aibase.com/news/18232                                                                                                                    | At the I/O 2025 conference, Google officially unveiled Gemma3n, a multi-modal AI model designed specifically for low-resource devices, capable of running smoothly on phones, tablets, and laptops with just 2GB of RAM.                                                                                                                                                                         |\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(tool_call_result.content)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "231a4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_parse_result = await client.invoke_tool('parse_url',{'url':'https://codersera.com/blog/install-and-run-gemma-3n-locally-a-complete-guide '})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "633bf6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply as a Freelancer\n",
      "Unfortunately, the page you‚Äôre looking for doesn‚Äôt exist.\n",
      "Go to Home Page\n",
      "Hire a Developer\n",
      "Work with top freelancers on a no-risk trail period.\n",
      "Become a Developer\n",
      "Apply to join the elite ranks of free lancers\n",
      "Read World-class research and insights in business,design,technology\n",
      "See answers to common freelancing questions\n",
      "Get in touch with our team\n",
      "Based on 1095 Ratings\n",
      "Hire a Developer\n",
      "Apply as a developer\n",
      "Write For Us\n",
      "Partner With Us\n",
      "Hire Node.JS Developer\n",
      "Hire React Developer\n",
      "Hire React Native Developer\n",
      "Hire Express.JS Developer\n",
      "Hire Javascript Developer\n",
      "Hire Angular Developer\n",
      "Hire Typescript Developer\n",
      "Looking for Job\n",
      "Terms of services\n",
      "7 Day Free Trial Policy\n",
      "Clipy: screen recorder\n",
      "404 Link Checker\n",
      "Browser Note Taker\n",
      "AI Image Editor\n",
      "AI Resume Builder\n",
      "¬© 2025 Codesera Rights Reserved\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(tool_parse_result.content)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d075f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb31ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548236f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19528d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
