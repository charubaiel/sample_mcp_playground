{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4168a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e33322",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed324880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redditwarp.SYNC as reddit_SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983f3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = reddit_SC.Client()\n",
    "def reddit_search(seqrch_query:str,amount:int=10,subreddit:str='',min_comments_cnt:int=5,min_post_score:int=10,**kwargs):\n",
    "    search_response = client.p.submission.search(sr=subreddit,query=seqrch_query,amount=amount*2,**kwargs)\n",
    "    posts_data = [client.p.comment_tree.fetch(post.id) for post in search_response]\n",
    "    posts = [post for post in posts_data if (post.value.comment_count>min_comments_cnt) and (post.value.score > min_post_score)]\n",
    "    post_comments = { post.value.title:\n",
    "                        {'comments':[comment.value.body for comment in post.children],\n",
    "                        'score':post.value.score,\n",
    "                        'comment_count':post.value.comment_count,\n",
    "                        }\n",
    "                    for post in posts[:amount]}\n",
    "    return post_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2920d4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'When to Fine-Tune LLMs (and When Not To) - A Practical Guide': {'comments': [\"While I get that things that can be solved with rag should not be fine tuned. What about fine tuning for base knowledge for rag? Say we have a complex project with its own vocabulary therefore model has no knowledge of it and of similar tools. Now I fine-tune the model to have a grasp of the project so it could produce better outputs in RAG. Does this make sense or it's better to prompt and rag regular model?\",\n",
       "   'I hardly have anywhere near your experience or knowledge in fine tuning. I’ve been tinkering with fine tuning for just over the past six months. So I’m definitely not refuting anything you wrote and I appreciate the informative write up!\\n\\nI will (very humbly) say that I somewhat disagree with you regarding fine tuning’s effectiveness in adding new knowledge to a base model. \\n\\nI’ve had what I would call measurably good results using SFT+RL(PPO) for adding new knowledge to a base model. \\n\\nNow, obviously I was teaching it brand new universal laws of physics. \\n\\nBut for example, teaching a model a new language it wasn’t trained on and getting it to produce as good (or almost as good )output as the languages it was trained on - that can work pretty well in my very limited experience.',\n",
       "   \"Just a message to thank you for your awesome work !\\n\\nCurrently building a complex financial automation system using LLM, and I know that at a point I'll need to fine-tune models to improve cost, efficiency and precision (at the moment I'm still focused on my workflow, Nifi, Kafka...).\\n\\nI've been following the development and playing a bit with Kiln and it's really well made.\\n\\nThe only missing thing is a native docker image, because Kubernetes you know :)\",\n",
       "   'When talking about fine tuning I feel like a distinction needs to be made between the type of base models - based on whether they’re already instruction tuned or not. Do you tend to use and/or recommend one over the other?',\n",
       "   '.cursorrule #1:\\n\\n>  - call me “boss”\\n\\nI lolled',\n",
       "   \"Well I somewhat agree but let's say you tried rag and it wasn't enough on a specific framework, after fine-tuning + Rag I achieved results of 89% on eval made by that framework evening second place and testing modell manually also yielded much, much better results then just RAG. Wouldn't you say fine-tuning for a specific domain and mostly that domain is a viable strategy? There is also metods like RAFT for enchansing RAG.\",\n",
       "   'Thanks OP. My concern with Fine tuning is the ROI for me investing the resources to fine tune a model versus waiting for someone else to produce a better model. If I fine tune I have effectively tightly coupled my capability with a now very specific model making it harder for me to adopt a new model that may have better potential (I would lose all of the fine tuning efforts I’ve done).\\nCan you help either correct where my assumption are wrong with the above or share some insight on how you approach that ROI problem?',\n",
       "   'i am doing a discovery and curious about how people handle controls and guardrails for LLMs / Agents for more enterprise or startups use cases / environments.\\n\\n* How do you balance between limiting bad behavior and keeping the model utility?\\n* What tools or methods do you use for these guardrails?\\n* How do you maintain and update them as things change?\\n* What do you do when a guardrail fails?\\n* How do you track if the guardrails are actually working in real life?\\n* What hard problem do you still have around this and would like to have a better solution?\\n\\nWould love to hear about any challenges or surprises you’ve run into. Really appreciate the comments! Thanks!',\n",
       "   \"I accidentally fine tuned one to just be a chair..to see what would happen…now whatever I message it only replies are like a creaking symphony of onomatopoeia…::krrNNNk:: .... gr—kk—tCH — eeeeeeeeeee — T'knk—t'knk—t'knk\",\n",
       "   \"Wow, this is an incredibly comprehensive guide on fine-tuning LLMs! As someone who's been experimenting with AI chatbots, I've found that fine-tuning can really make or break the user experience. Your point about using it for style conformance resonates with me - I've been using Chat Data to create customer support bots, and fine-tuning has been crucial for nailing that professional-yet-friendly tone. It's amazing how much more natural and on-brand the interactions feel now. I'm curious, have you found any particular challenges when fine-tuning for very specific industry jargon or technical language? That's an area where I'm still trying to optimize performance.\",\n",
       "   \"For RAG I am 100% with you however what about fine tuning embeddings and reranking model ?\\nAlso if you have any specific advice on fine tuning to add citation ability to models for RAG so that they learn to cite specific context chunks used to generate specific sentences or refuse to answer instead of hallucinating, I 'd be very interested!\\nThx.\",\n",
       "   'I have a dataset of images and I want to classify them by document type and language. Can fine-tuning a model (Gemma3-4B) help me achieve this, given that my desired output for each image is its \"document_type\" and \"language_code\"?',\n",
       "   'Does your tool work with LoRa, or is it performing full fine-tuning?',\n",
       "   \"Would you say it's even realistic to fine-tune 8B+ models? What's the timeline or compute requirements for something like that? I can definitely see use cases that would benefit. Just not sure if it's worth the time or money if a new 8B or smaller model is going to come out in the next 6 months and be just as good for my use case as the fine-tune I just spent time improving.\",\n",
       "   'Thank you Sir for the great content quality you have here. I want to start fine tune some models and is always good to read this kind of content before starting',\n",
       "   '\\n\\nThanks for sharing! When it comes to building a coding  expert  for understanding large codebases, would you recommend fine-tuning over RAG-based systems ?',\n",
       "   'Commenting for future reference'],\n",
       "  'score': 122,\n",
       "  'comment_count': 40},\n",
       " 'Gemma 3n Architectural Innovations - Speculation and poking around in the model.': {'comments': ['> this model is stuffed to the brim with architectural innovations: Per-Layer Embedding (PLE), MatFormer Architecture, Conditional Parameter Loading.\\n\\n> There file TF_LITE_PER_LAYER_EMBEDDER contains very large lookup tables (262144x256x35) that will output a 256 embedding for every layer depending on the input token. Since this is essentially a lookup table, it can be efficiently processed even on the CPU. This is an extremely interesting approach to adding more capacity to the model without increasing FLOPS.\\n\\nI wonder if this was an experiment based on alphaevolve (or similar). Give the \"researcher agent\" a bunch of starting code, architecture ideas, efficiency goals, etc. and let it \"evolve\" model architectures. Train a few on small datasets, choose the best, evolve.step(). Take the best every n generations and train them on medium datasets to see where you\\'re at. Repeat.',\n",
       "   'Very interesting post, thank you!',\n",
       "   \"> The FFN is projecting from 2048 to 16384 with a GeGLU activation. This is an unusually wide ratio.\\n\\nInteresting. Gemma has changed this a lot over the generations:\\n\\n* [gemma-1.1-2b](https://huggingface.co/google/gemma-1.1-2b-it/blob/main/config.json#L11): model dim 2048, FFN dim 16384  (8x)\\n* [recurrentgemma-2b-it](https://huggingface.co/google/recurrentgemma-2b-it/blob/main/config.json): model dim 2560, FFN dim 15360 (6x)\\n* [gemma-2-2b](https://huggingface.co/google/gemma-2-2b/blob/main/config.json#L15): model dim 2304, FFN dim 9216 (4x)\\n* [gemma-3-1b](https://huggingface.co/google/gemma-3-1b-it/blob/main/config.json): model dim 1152, FFN dim 6912 (6x)\\n* [gemma-3-4b](https://huggingface.co/google/gemma-3-4b-it/blob/main/config.json): model dim 2560, FFN dim 10240 (4x)\\n\\nNot sure if there's any reason behind it. Maybe parameters are close enough to equivalence, no matter how dense they are, and they just made these choices while optimizing how to spread the model across TPUs...\\n\\nTBH, among these changes I'm surprised we haven't seen anything like Google's [Brainformers](https://arxiv.org/abs/2306.00008), which used 5 FFNs for every Attention layer, or NVIDIA's [Pay Attention when Required](https://arxiv.org/abs/2009.04534), which put more attention blocks at the start and more FFNs at the end.\",\n",
       "   'Does that mean I get a gguf file?\\nWanna run it on my computer',\n",
       "   'anyway to run this on iphone?',\n",
       "   \"These per-layer embeddings seem very interesting.\\n\\nI haven't looked in the code, but is the idea something like that you take a token, do a different embedding for every layer, add the hidden state, do positional encoding and then feed that into the dot-product attention?\",\n",
       "   \"fyi: Journeys for Android Studio\\xa0  \\nNot sure if they baked the model into Android Studio or if they run it in the Android Emulator. But I could imagine they're using the same model for the Android Test Journeys in the latest Canary of Android Studio.  \\n[https://developer.android.com/studio/preview/gemini/journeys?hl=en](https://developer.android.com/studio/preview/gemini/journeys?hl=en)\\n\\nhttps://preview.redd.it/fcfssxk8di4f1.png?width=2920&format=png&auto=webp&s=a876cab21c52d3762e85dbfab6ec602e77b257a7\",\n",
       "   'hey，I only find there are 30 layers，but you say there are 35？'],\n",
       "  'score': 176,\n",
       "  'comment_count': 23},\n",
       " \"I don't think gemma 3n is ready to ship\": {'comments': ['Blancas blancas blancas blancas. Blancas blancas blancas. Blancas blancas blancas.',\n",
       "   '🤣🤣\\n\\ngemma-3n-pro-tourettes-mini',\n",
       "   'https://preview.redd.it/p9oo8dphhw4f1.png?width=1178&format=png&auto=webp&s=8a902c53a625b2128f3e4fcc6d698a9bcf7972b4\\n\\nI got the same response from R1\\\\_Q2. I just ask what it thinks but it goes overly thinking.',\n",
       "   'Gemma 3n is an open-weights model that has been trained but not fine-tuned yet. This behaviour typically happens when an LLM needs to be fine-tuned. \\n\\nThis allows developers to fine-tune them for their own specific needs.',\n",
       "   'Lol',\n",
       "   \"That's my girl! 😚\",\n",
       "   'Wtf 😂',\n",
       "   'Blancas\\xa0',\n",
       "   'How did you get this other model?',\n",
       "   'Look at my last post it thought a car was a black square 😂😭',\n",
       "   \"Alright but what's Blancas ?\",\n",
       "   'I had this happen on gemini 2.5 pro where it kept repeating a phrase over n over again',\n",
       "   'Blancas',\n",
       "   'Blancas is the new Bazinga!',\n",
       "   \"Is this 2b or 4b? And give the model a break lol it's tiny\",\n",
       "   'white white white'],\n",
       "  'score': 61,\n",
       "  'comment_count': 26},\n",
       " 'On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!': {'comments': [\"Google's Edge Gallery app works on Galaxy S20+, too, at ~4 tokens per second...in case anyone needed to know that.\\n\\nClarifying: It can run Gemma 3n E4B.\",\n",
       "   'This is nice to see running Gemma 3n E4B on an old S21 Ultra is impressive!  \\nDid you need to quantize the model or tweak anything to make it smooth?\\n\\nThey are capable of multimodal input, handling text, image, video, and audio input, did you try those ?',\n",
       "   'This looks amazing! What app is this?',\n",
       "   'Somehow it keeps crashing on my galaxy s22+.',\n",
       "   'Can I import gguf files in it?'],\n",
       "  'score': 55,\n",
       "  'comment_count': 23},\n",
       " 'SFF books coming in November 2024': {'comments': [\"There will be lots of people waiting for **The Navigator's Children** (The Last King of Osten Ard 4) by **Tad Williams**.\\n\\nNew novels by **Ed McDonald**, **Elise Kova**, **K. B. Wagers**, **Melissa Caruso** and **Tasha Suri** amongst many others.\\n\\nI recently read an ARC of **Sleeping Worlds Have No Memory** by **Yaroslav Barsukov** and enjoyed it immensely. That's now being published on the 12th.\\n\\nI'll be adding these to my TBR:\\n\\n* And the Mighty Will Fall (NeoG 4) - K. B. Wagers (N) \\\\[tp\\\\]\\n* The Navigator's Children (The Last King of Osten Ard 4) - Tad Williams (N) \\\\[hc\\\\]\\n* This Inevitable Ruin (Dungeon Crawler Carl 7) - Matt Dinniman (N) \\\\[eb\\\\]\\n* Witch Queen of Redwinter (The Redwinter Chronicles 3) - Ed McDonald (N) \\\\[hc\\\\]\\n\\nWhich ones are you interested in?\\n\\nEdit: Added in Dungeon Crawler Carl.\",\n",
       "   'This Inevitable Ruin (Dungeon Crawler Carl 7) - Matt Dinniman\\n\\n11/11/24',\n",
       "   \"There's a whole series of smut about Santas Elves?   I ....  Idek.  I like romantasy enough but maybe it's getting a little out of control?   Lol\\n\\n\\n\\n\\nBut super excited for The Lotus Kingdom and latest DCC coming out a few days before my birthday.  Also pumped for the next Edinburgh Knights.  Didn't know there would be more in the series, so that will be fun too.\",\n",
       "   \"The two I'm definitely reading in November are The Lotus Empire by Tasha Suri and This Inevitable Ruin by Matt Dinniman.\\n\\nWill pick up The Relentless Legion J.S. Dewes when I'm in the mood for some SF.  I read the first two and thought they were solid but not amazing.\\n\\nAnd the Osten Ard Saga is staring at me, but I'm so rarely in the mood for epic fantasy. But as far as I can remember, the only Tad Williams' book I've read has been Tailchaser's Song, and that doesn't seem right.\",\n",
       "   'Could not be more excited for The Lotus Empire by Tasha Suri.',\n",
       "   \"November 12 :\\n\\n- Hell's Wardens (The Wandering's Inn 14) - Pirate Aba\\n\\nNovember 15 :\\n\\n- Memories of Tomorrow (The Tales of Aerym 1) - Noah Isaacs\\n\\nNovember 26 :\\n\\n- Titanchild (Talon 2) - Jen Williams\",\n",
       "   'Magical Girl Blues by Russell Isler (The Clandestine book 2) on November 26.',\n",
       "   \"I'm so grateful there aren't very many books coming out this month that I want to read (and just one book in December), my TBR is big enough as it is!\\n\\n11/12: The Moonstone Covenant by Jill Hammer  \\n11/12: Dead Girls Don't Dream by Nino Cipri   \\n11/12: The Keeper of the Key by Nicole Willson\\n\\nThere's two sequels to series I plan on reading but I haven't gotten around to yet, but someday I hope to read The Lotus Empire by Tasha Suri and I Am the Dark That Answers When You Call by Jamison Shea.\",\n",
       "   'Thank you once again.',\n",
       "   'Thank you for posting this! very helpful!',\n",
       "   \"Wow, that's quite a list. I don't immediately see one that I'm on the edge of my seat for, though a few are potentially intriguing. I do have one November ARC that I don't see here: We Are All Ghosts in the Forest by Lorraine Wilson\",\n",
       "   \"J.S Dewes' The Relentless Legion is what i'm eagerly waiting for!\\n\\n\\nI do love this series of very informational posts :)\",\n",
       "   'Oh, I thought there was a new Seven Swords by Anthony Ryan coming out again :('],\n",
       "  'score': 45,\n",
       "  'comment_count': 21},\n",
       " 'New music! Fresh tunes from Whiney, Levela, Fanu, Andromedik, Tim Reaper, Skylark, Skantia and more.. In review the new QZB album! [+weekly updated Spotify playlist] | New Music Monday! (Week 42)': {'comments': ['Thanks for including my new album 😘',\n",
       "   \"Bandcamp friday bringing it home with the Jungle this week!  \\n\\n\\nPersonally the liquid really shined this week. Laodes - Drinks & Nightlife, Satl - Call Me, Mitekiss - Your Ghosts & Channel - Peppermint were all super soothing and enjoyable. Rarely i get this many dope liquid tracks added to the library.  \\n\\n\\nFrom the deeper bangers  DLR, Ternion Sound, Give Back To Me (Black Barrel Remix) and Raiser - Make 'Em Bounce would be my picks. And ofcourse the QZB album. I'm yet to do a full listen but theres a for absolute heaters and vibers in there.  \\n\\n\\nAndromedik remix for Alesso was really good from the dancefloor side of things. And even though Dimensions new remix is pretty much a copy paste of DJ Turn It Up, its still a banger.\",\n",
       "   'What an impressive level of effort, thanks for sharing man',\n",
       "   'Thanks for the support <3',\n",
       "   'Thanks as always mate',\n",
       "   'First of all, great list as always. Looking forward to it every week :). Very much appreciated!\\n\\nI am missing the dancefloor section this week though, is it done on purpose or am I missing something?',\n",
       "   'That Ternion Sound album is so fucking nasty'],\n",
       "  'score': 28,\n",
       "  'comment_count': 13}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_search('What is gemma 3n?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54ec79",
   "metadata": {},
   "source": [
    "# DuckDuckGo Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60b3b943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Gemma 3n - Google DeepMind',\n",
       "  'href': 'https://deepmind.google/models/gemma/gemma-3n/',\n",
       "  'body': 'Gemma 3n is our state-of-the-art open multimodal model, engineered for on-device performance and efficiency. Google DeepMind. Google AI Learn about all of our AI Google DeepMind Explore the frontier of AI Google Labs Try our AI experiments Google Research Explore our research Gemini ...'},\n",
       " {'title': 'Gemma 3n - Каталог НЕЙРОСЕТЕЙ и ИИ инструментов — FutureTools.ru',\n",
       "  'href': 'https://futuretools.ru/tools/gemma-3n/',\n",
       "  'body': 'Gemma 3n использует инновационную архитектуру MatFormer, ... (2B и 4B параметров). Это означает, что модель может адаптироваться к конкретной задаче, не перегружая устройство. Начало работы с Gemma 3n.'},\n",
       " {'title': 'Google представила Gemma 3n — ИИ-модель, работающую с 2 ГБ памяти и без ...',\n",
       "  'href': 'https://overclockers.ru/blog/Global_Chronicles/show/225413/Google-predstavila-Gemma-3n-II-model-rabotajuschuju-s-2-GB-pamyati-i-bez-podkljucheniya-k-oblaku',\n",
       "  'body': 'На конференции I/O 2025 Google анонсировала новую многомодальную модель искусственного интеллекта Gemma 3n. Она способна обрабатывать текст, изображения, звук и видео прямо на устройстве с ограниченными ресурсами.'},\n",
       " {'title': 'Google представила Gemma 3n — лёгкую и быструю AI-модель для работы на ...',\n",
       "  'href': 'https://ai-news.ru/2025/05/google_predstavila_gemma_3n_legkuu_i_bystruu_ai_model_dlya_raboty_n.html',\n",
       "  'body': 'Google выпустила Gemma 3n — это новая версия модели, которая запускается локально на мобильных устройствах. Gemma 3n может работа локально на устройстве с 2 ГБ оперативной памяти! Особенности: • Работает в 1.5 раза быстрее, чем ...'},\n",
       " {'title': \"Gemma 3n: All about Google's open model for on-device AI on phones ...\",\n",
       "  'href': 'https://www.business-standard.com/technology/tech-news/gemma-3n-all-about-google-s-open-model-for-on-device-ai-on-phones-laptops-125052201070_1.html',\n",
       "  'body': 'Gemma 3n model: Key capabilities. Audio input: The model can process sound-based data, enabling applications like speech recognition, language translation, and audio analysis. Multimodal input: With support for visual, text, and audio inputs, the model can handle complex tasks that involve combining different types of data.'},\n",
       " {'title': 'Gemma 3n: Smarter, Faster, and Offline-Ready - KDnuggets',\n",
       "  'href': 'https://www.kdnuggets.com/gemma-3n-smarter-faster-and-offline-ready',\n",
       "  'body': \"Getting started with Gemma 3n is simple and accessible, with two primary methods available for developers to explore and integrate this powerful model. 1. Google AI Studio. To get started, simply log in to Google AI Studio, go to the studio, select the Gemma 3n E4B model, and begin exploring Gemma 3n's\"},\n",
       " {'title': 'What is Gemma 3n and How to Access it? - Analytics Vidhya',\n",
       "  'href': 'https://www.analyticsvidhya.com/blog/2025/05/gemma-3n/',\n",
       "  'body': \"When Gemma 3n models are executed, Per-Layer Embedding (PLE) settings are employed to generate data that improves each model layer's performance. As each layer executes, the PLE data can be created independently, outside the model's working memory, cached to quick storage, and then incorporated to the model inference process.\"},\n",
       " {'title': \"Gemma-3n: Google's AI Revolution for faster, CPU-friendly AI\",\n",
       "  'href': 'https://pub.towardsai.net/gemma-3n-googles-ai-revolution-for-faster-cpu-friendly-ai-7cc2a20a5fe8',\n",
       "  'body': \"Google's Gemma 3n. Imagine the power of cutting-edge AI, but right there on your device, private and always ready!That's exactly what Google unveiled on May 20, 2025, with Gemma 3n.This latest model in their open-source Gemma lineup is custom-made for mobile and edge devices, and it's a game-changer.It pushes AI beyond cloud-only limitations, bringing truly on-device intelligence for low ...\"},\n",
       " {'title': 'Gemma 3: Google DeepMind делает ИИ по-настоящему доступным',\n",
       "  'href': 'https://habr.com/ru/articles/890268/',\n",
       "  'body': 'Google DeepMind выкатила Gemma 3 - новое семейство открытых ИИ-моделей. И, судя по заявлениям, главный акцент сделан на доступность и производительность в реальных условиях. В эпоху, когда нейросети из экзотики превращаются в повседневный инструмент, такой подход выглядит особенно актуально.'},\n",
       " {'title': 'Google представила Gemma 3n — ИИ-модель, работающую на устройствах с 2 ...',\n",
       "  'href': 'https://teletype.in/@qbit_cat/9BQq4To01cH',\n",
       "  'body': 'Gemma 3n умеет работать с текстом, изображениями и аудио в рамках одного контекста до 32 000 токенов. Для обработки изображений используется визуальный энкодер SigLIP на 400 млн параметров, превращающий картинку в 256 ...'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DDGS().text('Модель gemma 3n',region='ru-ru',safesearch='off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a125f3",
   "metadata": {},
   "source": [
    "# Parse URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3024e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx \n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "def parse_url(url:str):\n",
    "    \"\"\" \n",
    "    Parse indeep info from web page. Get url and return all information than page have\n",
    "    Args:\n",
    "        url (str): url of web resource\n",
    "        amount (int): count posts to search\n",
    "\n",
    "    Returns:\n",
    "        Str: Raw text results from web page\n",
    "    \"\"\"\n",
    "    response = httpx.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup(['script', 'style', 'noscript', 'nav', 'footer', 'head', 'meta']):\n",
    "        element.decompose()\n",
    "\n",
    "    # Pre-process specific tags\n",
    "    for br in soup.find_all('br'):\n",
    "        br.replace_with('\\n')\n",
    "        \n",
    "    # List of block-level elements that should have line breaks\n",
    "    BLOCK_TAGS = ['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n",
    "                  'ul', 'ol', 'li', 'hr', 'tr', 'td', 'th', 'section',\n",
    "                  'article', 'header', 'table', 'pre']\n",
    "\n",
    "    # Add line breaks before and after block elements\n",
    "    for tag in soup.find_all(BLOCK_TAGS):\n",
    "        if tag.previous_sibling and not isinstance(tag.previous_sibling, NavigableString):\n",
    "            tag.insert_before('\\n')\n",
    "        if tag.next_sibling and not isinstance(tag.next_sibling, NavigableString):\n",
    "            tag.insert_after('\\n')\n",
    "\n",
    "    # Get text with proper spacing\n",
    "    text = soup.get_text(separator='\\n', strip=True)\n",
    "    \n",
    "    # Clean up excessive whitespace\n",
    "    text = '\\n'.join([line.strip() for line in text.splitlines() if line.strip() if len(line.split(' ')) > 2])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208da51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перейти к содержанию\n",
      "Google представила Gemma 3n: новая эра локальных AI-моделей\n",
      "Особенности Gemma 3n\n",
      "Начало работы с Gemma 3n\n",
      "Применение Gemma 3n\n",
      "Google представила Gemma 3n: новая эра локальных AI-моделей\n",
      "Компания Google продолжает совершенствовать свои AI-модели, выпуская новые версии, способные работать локально на устройствах. Последним достижением стала Gemma 3n — лёгкая и быстрая модель, предназначенная для локального запуска на мобильных устройствах.\n",
      "Особенности Gemma 3n\n",
      "Работает в 1.5 раза быстрее, чем предыдущая версия Gemma 3 4B\n",
      "Поддерживает работу без интернета, обеспечивая локальную обработку данных и повышенную безопасность\n",
      "Умеет понимать текст, речь и изображения, что делает её универсальной для различных приложений\n",
      "Может работать на устройствах с ограниченными ресурсами, такими как 2–3 ГБ RAM\n",
      "Поддерживает множество языков, расширяя её возможности для глобального использования\n",
      "Gemma 3n использует инновационную архитектуру MatFormer, которая позволяет “переключаться” между лёгким и полным режимом (2B и 4B параметров). Это означает, что модель может адаптироваться к конкретной задаче, не перегружая устройство.\n",
      "Начало работы с Gemma 3n\n",
      "Начать работать с Gemma 3n можно двумя способами:\n",
      "Через Google AI Studio, которая работает прямо в браузере\n",
      "Через SDK Google AI Edge, позволяющую интегрировать модель в Android, Chromebook и другие устройства\n",
      "Применение Gemma 3n\n",
      "Голосовые ассистенты, способные работать локально и быстро реагировать на запросы\n",
      "Приложения с ИИ, функционирующие без интернета, что особенно важно для безопасности и автономности\n",
      "Переводчики, чат-боты и системы анализа изображений на телефоне, расширяющие возможности мобильных устройств\n",
      "Более подробную информацию о Gemma 3n и её возможностях можно найти в\n",
      "No-code specialist, always eager to learn and tackle challenges, exploring neural networks\n",
      "Как создать профессиональную чёрно-белую аватарку в ChatGPT за минуту\n",
      "Самое важное из первого подкаста OpenAI: AGI, GPT-5 и будущее ИИ\n",
      "Корпоративные шахматы: Как Сэм Альтман вытесняет Microsoft из OpenAI\n",
      "Изучите похожие инструменты искусственного интеллекта\n",
      "Компания Manus представила инновационный продукт —\n",
      "Как создать профессиональную чёрно-белую аватарку в ChatGPT за минуту\n",
      "Инструкции по созданию профессиональных аватарок с\n",
      "Команда MiniMax сделала значительный шаг в развитии\n",
      "Самое важное из первого подкаста OpenAI: AGI, GPT-5 и будущее ИИ\n",
      "Недавно OpenAI запустил собственный подкаст, и первым\n",
      "Корпоративные шахматы: Как Сэм Альтман вытесняет Microsoft из OpenAI\n",
      "Гениальная многоходовка или корпоративный переворот?\n",
      "MiniMax-M1: Открытая Reasoning-модель с рекордным контекстом 1\n",
      "RoboBrain 2.0 открывает новую эру в робототехнике как\n",
      "Автоматизировать рабочие процессы стало проще благодаря\n",
      "Этот веб-сайт использует файлы cookie для улучшения взаимодействия с пользователем. Продолжая пользоваться сайтом, вы даете согласие на использование файлов cookie.\n"
     ]
    }
   ],
   "source": [
    "print(parse_url('https://futuretools.ru/tools/gemma-3n/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f7d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc6660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
